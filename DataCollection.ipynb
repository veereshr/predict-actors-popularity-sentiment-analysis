{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import jsonpickle\n",
      "import os\n",
      "import tweepy\n",
      "import ConfigParser"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Tweepy is used for data collection.\n",
      "Twitter is being authenticated using tweepy auth handler by providing the consumer_key and consumer_secret keys.\n",
      "'''\n",
      "\n",
      "config = ConfigParser.ConfigParser()\n",
      "config.read(os.getcwd()+'/twitter.cfg')\n",
      "\n",
      "\n",
      "# Replace the API_KEY and API_SECRET with your application's key and secret.\n",
      "auth = tweepy.AppAuthHandler(config.get('twitter', 'consumer_key'), config.get('twitter', 'consumer_secret'))\n",
      " \n",
      "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
      "\t\t\t\t   wait_on_rate_limit_notify=True)\n",
      " \n",
      "if (not api):\n",
      "    print (\"Can't Authenticate\")\n",
      "    sys.exit(-1)\n",
      " \n",
      "# Continue with rest of code\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n",
        "  InsecurePlatformWarning\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "'''\n",
      "Regions are defined.\n",
      "Creating directory to download data accordingly as each actor and appropriate regions.\n",
      "'''\n",
      "\n",
      "regions = [\"SouthWest\",\"MidWest\",\"NorthEast\",\"West\",\"SouthEast\"]\n",
      "\n",
      "filename = open(\"data/actors/actors.txt\",'r').readlines()\n",
      "actor_names = []\n",
      "for line in filename:\n",
      "    actor_names.append(line.rstrip(\"\\n\"))\n",
      "\n",
      "for actor in actor_names:\n",
      "        if not os.path.exists(\"data/tweets/test/\"+actor):\n",
      "            os.makedirs(\"data/tweets/test/\"+actor)\n",
      "for actor in actor_names:\n",
      "    for region in regions:\n",
      "        if not os.path.exists(\"data/tweets/test/\"+actor+\"/\"+region):\n",
      "            os.makedirs(\"data/tweets/test/\"+actor+\"/\"+region)            \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Using geopy, state name of the user location is fetched and then region is mapped accordingly\n",
      "'''\n",
      "from geopy.geocoders import Nominatim\n",
      "geolocator = Nominatim()\n",
      "\n",
      "    \n",
      "midwest_states = open(\"data/states/midwest.txt\", 'r').readline().rstrip(\"\\n\").split(\",\")\n",
      "northeast_states = open(\"data/states/northeast.txt\", 'r').readline().rstrip(\"\\n\").split(\",\")\n",
      "southeast_states = open(\"data/states/southeast.txt\", 'r').readline().rstrip(\"\\n\").split(\",\")\n",
      "southwest_states = open(\"data/states/southwest.txt\", 'r').readline().rstrip(\"\\n\").split(\",\")\n",
      "west_states = open(\"data/states/west.txt\", 'r').readline().rstrip(\"\\n\").split(\",\")\n",
      "\n",
      "\n",
      "def get_region(address):\n",
      "    \n",
      "    location = geolocator.geocode(address,timeout=100)\n",
      "    region = \"\"\n",
      "    \n",
      "    if location != None:\n",
      "        raw_location = location.raw['display_name'].split(\", \")\n",
      "        for x in southwest_states:\n",
      "            if x in raw_location:\n",
      "                region = \"SouthWest\"\n",
      "                break\n",
      "        for x in midwest_states:\n",
      "            if x in raw_location:\n",
      "                region = \"MidWest\"\n",
      "                break\n",
      "        for x in northeast_states:\n",
      "            if x in raw_location:\n",
      "                region = \"NorthEast\"\n",
      "                break\n",
      "        for x in west_states:\n",
      "            if x in raw_location:\n",
      "                region = \"West\"\n",
      "                break\n",
      "        for x in southeast_states:\n",
      "            if x in raw_location:\n",
      "                region = \"SouthEast\"\n",
      "                break\n",
      "    return region\n",
      "get_region(\"Sheridan\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "'MidWest'"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Tweepy is used for tweet collection.\n",
      "Various checks such as not including retweets, and checking the user location such as finding whether the user\n",
      "has used correct address, mapping the user state to particular region,etc.,\n",
      "'''\n",
      "def get_tweets( search_query, max_tweets=10000):\n",
      "    searchQuery = search_query  # this is what we're searching for\n",
      "    maxTweets = max_tweets # Some arbitrary large number\n",
      "    tweetsPerQry = 100  # this is the max the API permits\n",
      "    #fName = search_query+'.txt' # We'll store the tweets in a text file.\n",
      "    \n",
      "    \n",
      "    # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
      "    # else default to no lower limit, go as far back as API allows\n",
      "    sinceId = None\n",
      "    \n",
      "    # If results only below a specific ID are, set max_id to that ID.\n",
      "    # else default to no upper limit, start from the most recent tweet matching the search query.\n",
      "    max_id = -1L\n",
      "    \n",
      "    tweetCount = 0\n",
      "    print(\"Downloading max {0} tweets\".format(maxTweets))\n",
      "    i = 0\n",
      "    while tweetCount < maxTweets:\n",
      "        try:\n",
      "            if (max_id <= 0):\n",
      "                if (not sinceId):\n",
      "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry, lang=\"en\")\n",
      "                else:\n",
      "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
      "                                            since_id=sinceId, lang=\"en\")\n",
      "            else:\n",
      "                if (not sinceId):\n",
      "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
      "                                            max_id=str(max_id - 1), lang=\"en\")\n",
      "                else:\n",
      "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
      "                                            max_id=str(max_id - 1),\n",
      "                                            since_id=sinceId, lang=\"en\")\n",
      "            if not new_tweets:\n",
      "                print(\"No more tweets found\")\n",
      "                break\n",
      "            location = \"\"\n",
      "            for tweet in new_tweets:\n",
      "                location = tweet.user.location\n",
      "                if location != \"\":\n",
      "                    region = get_region(location)\n",
      "                    if region != \"\":\n",
      "                        if not 'retweeted_status' in tweet._json:\n",
      "                            fName = \"data\"+os.sep+\"tweets\"+os.sep+\"testX\"+os.sep+search_query+os.sep+region+os.sep+search_query+`i`+'.txt'\n",
      "                            with open(fName, 'w') as f:\n",
      "                                f.write(jsonpickle.encode(tweet._json['text'], unpicklable=False) +'\\n')\n",
      "                                i+=1\n",
      "                                tweetCount += 1\n",
      "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
      "            max_id = new_tweets[-1].id\n",
      "        except tweepy.TweepError as e:\n",
      "            # Just exit if any error\n",
      "            print(\"some error : \" + str(e))\n",
      "            break\n",
      "    \n",
      "    print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "For each actor in the actors file list, unique tweets are being fetched.\n",
      "'''\n",
      "for actor in actor_names:\n",
      "    get_tweets(actor, max_tweets=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Creating directories for test data according to the structure.\n",
      "Later on, data is manually categorized as positive and negative.\n",
      "'''\n",
      "\n",
      "for x in actor_names:\n",
      "    if not os.path.exists(\"data\"+os.sep+\"test\"+os.sep+x):\n",
      "        os.makedirs(\"data\"+os.sep+\"test\"+os.sep+x)\n",
      "    for y in regions:\n",
      "        if not os.path.exists(\"data\"+os.sep+\"tweets\"+os.sep+\"test\"+os.sep+x+os.sep+y):\n",
      "            os.makedirs(\"data\"+os.sep+\"tweets\"+os.sep+\"test\"+os.sep+x+os.sep+y)\n",
      "        if not os.path.exists(\"data\"+os.sep+\"tweets\"+os.sep+\"test\"+os.sep+x+os.sep+y+os.sep+\"pos\"):\n",
      "            os.makedirs(\"data\"+os.sep+\"tweets\"+os.sep+\"test\"+os.sep+x+os.sep+y+os.sep+\"pos\")\n",
      "        if not os.path.exists(\"data\"+os.sep+\"tweets\"+os.sep+\"test\"+os.sep+x+os.sep+y+os.sep+\"neg\"):\n",
      "            os.makedirs(\"data\"+os.sep+\"tweets\"+os.sep+\"test\"+os.sep+x+os.sep+y+os.sep+\"neg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    }
   ],
   "metadata": {}
  }
 ]
}